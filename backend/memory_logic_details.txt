## Deep Dive into Memory Management Logic (with Prompts)

This document provides an in-depth explanation of how the system handles memories, including their ingestion, extraction, indexing, and retrieval. We will cover the key services involved and their interactions, illustrating with relevant code snippets and, crucially, the exact prompts used for LLM interactions.

### I. Memory Service (`memory.service.ts`)

The `MemoryService` acts as the primary interface for creating and retrieving user memories. It orchestrates the process of saving raw content, generating embeddings, and queuing further processing.

**1. Ingesting a New Memory (`ingest` function)**

When new content (e.g., a user's note, a chat message) needs to be stored as a memory, the `ingest` function is called.

**Logic:**
*   Saves the raw memory content to the database.
*   Generates a vector embedding for the memory content.
*   Adds the memory and its embedding to the memory index for efficient retrieval.
*   Queues the memory for asynchronous knowledge triplet extraction.

**Code Snippet (Conceptual - `memory.service.ts`):**
```typescript
import prisma from '../db';
import { llmService } from './llm.service';
import { memoryIndexService } from './memory-index.service';
import { memoryQueue } from '../queues/memory.queue';

class MemoryService {
  async ingest(
    userId: string,
    content: string,
    type: string,
    importance: number,
    source: string,
    recordedAt: string | null
  ) {
    // 1. Save memory to database
    const newMemory = await prisma.memory.create({
      data: {
        userId,
        content,
        type,
        importance,
        source,
        recordedAt,
        // ... other fields
      },
    });

    // 2. Generate embedding and add to memory index (asynchronous)
    // The actual embedding generation and indexing might be handled within memoryIndexService
    await memoryIndexService.addMemory(newMemory.id, userId, content);

    // 3. Queue for knowledge triplet extraction (asynchronous)
    memoryQueue.add('extract', {
      memoryId: newMemory.id,
      userId: userId,
      content: content,
      type: newMemory.type,
    }).catch(err => console.error('Error queuing memory for extraction:', err));

    return newMemory;
  }
  // ... other methods
}
```

**2. Retrieving Memories (`retrieve` function)**

This function is responsible for finding and returning memories relevant to a given query.

**Logic:**
*   Uses the `memoryIndexService` to perform a similarity search based on the query.
*   Fetches the full details of the top-N relevant memories from the database.
*   (Historically, it might have involved `reasoningService`, but now `smartCacheService` in `chat.service.ts` handles complex reasoning.)

**Code Snippet (Conceptual - `memory.service.ts`):**
```typescript
import { memoryIndexService } from './memory-index.service';

class MemoryService {
  // ... ingest function ...

  async retrieve(userId: string, query: string, limit: number = 5) {
    // 1. Search for relevant memories using the index
    const relevantMemories = await memoryIndexService.searchMemories(userId, query, limit);

    // 2. Fetch full details of these memories from the database
    const memoryDetails = await prisma.memory.findMany({
      where: {
        id: { in: relevantMemories.map(m => m.id) },
        deleted: false,
      },
      select: {
        id: true,
        content: true,
        type: true,
        recordedAt: true,
      },
    });

    // Format and return
    return memoryDetails.map(mem => `[${mem.type} - ${mem.recordedAt || 'N/A'}] ${mem.content}`);
  }
}
```

### II. Memory Index Service (`memory-index.service.ts`)

This service is responsible for managing the vector index of memories, enabling fast and efficient semantic search. It abstracts the underlying vector database (e.g., Pinecone, ChromaDB, or a custom in-memory solution).

**1. Adding a Memory to the Index (`addMemory` function)**

**Logic:**
*   Generates a vector embedding for the memory's content using `llmService.createEmbedding()`.
*   Stores this embedding along with the memory's ID and `userId` in the vector index.

**Code Snippet (Conceptual - `memory-index.service.ts`):**
```typescript
import { llmService } from './llm.service';
// Assume a vector database client is initialized here, e.g., pineconeClient

class MemoryIndexService {
  async addMemory(memoryId: string, userId: string, content: string) {
    // 1. Generate embedding
    const embedding = await llmService.createEmbedding(content);

    // 2. Store in vector index (conceptual interaction with a vector DB)
    // Example: await pineconeClient.index('memories').upsert({
    //   id: memoryId,
    //   values: embedding,
    //   metadata: { userId: userId }
    // });
    console.log(`[MemoryIndexService] Added memory ${memoryId} to index.`);
  }
  // ... other methods
}
```

**2. Searching Memories in the Index (`searchMemories` function)**

**Logic:**
*   Generates a vector embedding for the search query.
*   Queries the vector index to find memories with embeddings most similar to the query embedding.
*   Filters results by `userId` to ensure privacy and relevance.

**Code Snippet (Conceptual - `memory-index.service.ts`):**
```typescript
import { llmService } from './llm.service';

class MemoryIndexService {
  // ... addMemory function ...

  async searchMemories(userId: string, query: string, topK: number = 5, excludeIds: string[] = []) {
    // 1. Generate embedding for the query
    const queryEmbedding = await llmService.createEmbedding(query);

    // 2. Query the vector index (conceptual interaction with a vector DB)
    // Example: const results = await pineconeClient.index('memories').query({
    //   vector: queryEmbedding,
    //   topK: topK,
    //   filter: { userId: { '$eq': userId }, id: { '$nin': excludeIds } }
    // });

    // For demonstration, simulate results
    const simulatedResults = [
      { id: 'mem1', score: 0.9 },
      { id: 'mem2', score: 0.85 },
      // ...
    ];

    console.log(`[MemoryIndexService] Searched memories for user ${userId}, found ${simulatedResults.length} results.`);
    return simulatedResults.map(r => ({ id: r.id, score: r.score }));
  }
}
```

### III. Memory Extractor Service (`memory-extractor.service.ts`)

This service is responsible for taking raw content (like chat messages) and extracting more structured, atomic memories from it. It's often triggered by the `memoryQueue`.

**1. Extracting and Storing Memories (`extractAndStoreMemories` function)**

**Logic:**
*   Constructs a prompt for the LLM to extract distinct memories from a given text.
*   Sends this prompt to `llmService.generateCompletion()`.
*   Parses the LLM's response, which is expected to be a JSON array of extracted memories.
*   For each extracted memory:
    *   Saves it to the database via `memoryService.ingest()`.
    *   Performs contradiction detection (`contradictionDetectionService.detectContradictions()`).
*   **Crucially, invalidates the cache for any entities mentioned in the newly extracted memories.** This ensures that `smartCacheService` will re-compute insights with the latest information.

**Code Snippet (Conceptual - `memory-extractor.service.ts`):**
```typescript
import { llmService } from './llm.service';
import { memoryService } from './memory.service';
import { ContradictionDetectionService } from './contradiction-detection.service';
import { smartCacheService } from './smart-cache.service';

interface ExtractedMemory {
  type: string;
  content: string;
  importance: number;
  entities?: string[]; // Entities detected in this memory
}

class MemoryExtractorService {
  private contradictionDetectionService: ContradictionDetectionService;

  constructor() {
    this.contradictionDetectionService = new ContradictionService();
  }

  async extractAndStoreMemories(userId: string, text: string, source: string) {
    // The exact prompt used for memory extraction
    const prompt = `From the following text, extract distinct, atomic memories. Each memory should be a concise, factual statement. Return a JSON array of objects, where each object has 'type' (e.g., 'fact', 'event', 'thought'), 'content', 'importance' (0-1), and optionally 'entities' (an array of strings).

Text:
${text}

Memories (JSON array):`;

    const llmResponse = await llmService.generateCompletion(prompt);
    const extractedMemories: ExtractedMemory[] = JSON.parse(llmResponse); // Error handling omitted for brevity

    const detectedEntitiesInBatch = new Set<string>();

    for (const mem of extractedMemories) {
      const newMemory = await memoryService.ingest(
        userId,
        mem.content,
        mem.type,
        mem.importance,
        source,
        null // recordedAt
      );

      // Perform contradiction detection (asynchronous)
      this.contradictionDetectionService.detectContradictions(userId, newMemory.id, newMemory.content)
        .catch(err => console.error('Contradiction detection failed:', err));

      // Collect entities for cache invalidation
      if (mem.entities) {
        mem.entities.forEach(entity => detectedEntitiesInBatch.add(entity));
      }
    }

    // Invalidate cache for all entities detected in this batch of extracted memories
    for (const entityName of detectedEntitiesInBatch) {
      await smartCacheService.invalidateEntity(entityName);
    }
  }
}
```

### IV. Interaction within the Chat Flow (`chat.service.ts`)

The `ChatService` integrates these memory components at key points:

1.  **Phase 1: Memory Search:**
    *   `memoryIndexService.searchMemories(userId, message, ...)` is called to find relevant memories that will be included in the LLM prompt.
    *   **Code Snippet (from `chat.service.ts`):**
        ```typescript
        const searchMemoriesPromise = memoryIndexService.searchMemories(userId, message, 5, []);
        const [history, contextEntities, relevantMemories] = await Promise.all([
          historyPromise,
          entitiesPromise,
          searchMemoriesPromise, // This is where memory search happens
          saveMessagePromise
        ]);
        ```

2.  **Phase 4: Post-Response Memory Extraction:**
    *   After the assistant's response is generated and saved, both the user's message and the assistant's response are queued for memory extraction. This ensures that new information or insights from the conversation are captured and added to the user's knowledge base.
    *   **Code Snippet (from `chat.service.ts`):**
        ```typescript
        // Queue memory extraction
        memoryQueue.add('extract', {
          userId, chatId,
          userMessage: message,
          assistantMessage: fullResponse
        }).catch(err => console.error('[ChatService] Queue error:', err));
        ```

### V. Interaction with Background Jobs

1.  **Triplet Extraction Job (`triplet_extraction.job.ts`):**
    *   This job is the primary consumer of the `extract` jobs from the `memoryQueue`.
    *   It takes the raw content (user message + assistant response, or a standalone memory) and uses `llmService.generateCompletion()` with a specific prompt to extract knowledge triplets.
    *   These triplets are then used to build and update the knowledge graph (entities and entity links) in the database.
    *   **The exact prompt used for triplet extraction:**
        ```
        From the following text, extract knowledge triplets in the format of a JSON array of objects: [{ "subject": "", "predicate": "", "object": "" }].
        For each triplet, also include the "sourceId" and "sourceType" from the provided context. The sourceId is "${contentItem.id}" and sourceType is "${contentItem.type}".
        Focus on factual information, relationships between entities, and key actions. Ensure subjects and objects are specific entities.

        Text:
        ${contentItem.content}

        Triplets (JSON array):
        ```
    *   **Code Snippet (from `triplet_extraction.job.ts` - prompt example):**
        ```typescript
        const prompt = `From the following text, extract knowledge triplets in the format of a JSON array of objects: [{ "subject": "", "predicate": "", "object": "" }].\nFor each triplet, also include the "sourceId" and "sourceType" from the provided context. The sourceId is "${contentItem.id}" and sourceType is "${contentItem.type}".\nFocus on factual information, relationships between entities, and key actions. Ensure subjects and objects are specific entities.\n\nText:\n${contentItem.content}\n\nTriplets (JSON array):`;
        const llmResponse = await llmService.generateCompletion(prompt);
        // ... parsing and database updates ...
        ```

2.  **Summarization Job (`summarization.job.ts`):**
    *   This job aggregates multiple memories over a period (e.g., 24 hours) and uses `llmService.generateCompletion()` to create a concise summary.
    *   **The exact prompt used for summarization:**
        ```
        Please summarize the following collection of memories into a concise, coherent daily summary. Focus on key events, decisions, and insights. The summary should be in the first person, as if the user is recalling their day.

        Memories:
        ${combinedContent}

        Daily Summary:
        ```
    *   **Code Snippet (from `summarization.job.ts` - prompt example):**
        ```typescript
        const prompt = `Please summarize the following collection of memories into a concise, coherent daily summary. Focus on key events, decisions, and insights. The summary should be in the first person, as if the user is recalling their day.

Memories:
${combinedContent}

Daily Summary:`;
        const summaryContent = await llmService.generateCompletion(prompt);
        // ... saving summary ...
        ```

This intricate interplay of services ensures that memories are effectively captured, processed, and utilized throughout the system, building a comprehensive and dynamic knowledge base.
