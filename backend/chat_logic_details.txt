## Deep Dive into Chat Function Logic, Caching, and Memory Extraction Prompts

This document provides an in-depth explanation of the core logic governing chat interactions, the caching mechanisms employed for efficiency, and the specific prompts used for extracting structured memories.

### I. Chat Function Logic (`chat.service.ts`)

The `ChatService` orchestrates the entire lifecycle of a chat message, from user input to the streamed LLM response. It's designed for speed and intelligence, broken down into four distinct phases:

**Phase 1: Parallel Data Gathering (Critical Path)**
This phase executes several critical, independent tasks concurrently to minimize initial latency:
1.  **User Message Persistence:** The incoming user message is immediately saved to the `chat_messages` database table.
2.  **Chat History Retrieval:** The most recent messages for the current chat session are fetched, providing conversational context.
3.  **Quick Entity Extraction:** A lightweight, regex-based process quickly identifies potential key entities (capitalized words/phrases) from the user's message. This is a fast, non-LLM operation.
4.  **Memory Search:** The `memoryIndexService` performs a vector similarity search to retrieve the most semantically relevant memories from the user's knowledge base.

**Phase 2: Memory Details Retrieval**
Once the IDs of relevant memories are identified in Phase 1, their full content and associated metadata are fetched from the database. This phase is kept lean to avoid any computationally expensive operations.

**Phase 3: Smart Context Building with Caching**
This is the brain of the chat function, where the system intelligently decides how to construct the LLM prompt based on query complexity and available insights.
1.  **Query Analysis (`queryAnalyzerService`):**
    *   The user's message is analyzed by `queryAnalyzerService.analyzeQuery()`. This service uses pattern matching (e.g., regex for keywords like "timeline of", "relationship between") to determine if the query is `isComplex` and what type of insights are needed (`needsGraph`, `needsTimeline`). It also extracts entities from the query.
    *   **Logic:** If the query matches patterns for timeline, relationship, or analysis, `isComplex` is set to `true`. Otherwise, it's considered a `simple` query.
2.  **Conditional Insight Loading (`smartCacheService`):**
    *   **For Complex Queries:** If `queryAnalysis.isComplex` is `true` and entities are detected, the system attempts to retrieve pre-computed "insights" (graph relationships, timeline narratives) for these entities from the `smartCacheService` (backed by Redis).
        *   **Cache Hit:** If insights are found in the cache, they are used directly.
        *   **Cache Miss (Lazy Computation):** If not cached, `smartCacheService.lazyComputeAndCache()` is invoked. This function dynamically computes the required insights:
            *   If `needsGraph` is true, it calls `graphService.getRelationships()` to build the entity's relationship graph.
            *   If `needsTimeline` is true, it calls `reasoningService.buildTimeline()` to generate a narrative timeline.
            *   These newly computed insights are then stored in the cache for future use and the entity's usage is tracked.
    *   **For Simple Queries:** If `queryAnalysis.isComplex` is `false`, this entire complex insight loading process is skipped, and the `reasoningContext` remains empty.
3.  **Reasoning Context Construction:** Any retrieved or computed insights are formatted into a `reasoningContext` string, which will be injected into the LLM's system prompt.
4.  **Dynamic Prompt Generation:**
    *   **System Prompt:** The `systemPrompt` is dynamically tailored based on `queryAnalysis.isComplex`:
        *   **Simple Prompt:** A concise instruction for the LLM to act as a helpful assistant, use the personal knowledge base, format answers in MDX, and cite sources. It does not include the `reasoningContext`.
        *   **Complex Prompt:** An enriched prompt that includes the `reasoningContext` (if available). It instructs the LLM to use these insights for more helpful, proactive answers, offer actions, mention connections, and adhere to specific MDX formatting with custom components (`<DateHighlight>`, `<MemoryHighlight>`).
    *   **User Prompt:** This prompt combines the `Relevant Memories` (from Phase 2), `Chat History` (from Phase 1), and the `User's Question` into a single input for the LLM.
5.  **Final Prompt Assembly:** The chosen `systemPrompt` and the `userPrompt` are concatenated to form the complete input for the LLM.

**Phase 4: Stream Response**
1.  **LLM Stream Initiation:** The final prompt is sent to `llmService.generateCompletionStream()` (using `qwen2.5:1.5b` for streaming), initiating a real-time streamed response from the LLM.
2.  **Response Streaming:** The LLM's output is streamed back to the user.
3.  **Assistant Message Persistence:** The complete assistant's response is saved to the `chat_messages` table once the stream concludes.
4.  **Metrics Tracking:** `metricsService.trackQuery()` records various metrics (e.g., query complexity, cache hit status, response time) for analysis.
5.  **Memory Extraction Queueing:** A job is added to the `memoryQueue` to asynchronously extract knowledge triplets from both the user's message and the assistant's response.

### II. Caching Logic (`smartCacheService` and `smart-precompute.job.ts`)

The caching system is designed to optimize performance for complex queries by storing and reusing computationally expensive insights.

1.  **`SmartCacheService`:**
    *   **`getCachedInsights(entityName)`:** Performs a fast lookup in Redis for insights related to a given entity. If found, it's a cache hit.
    *   **`lazyComputeAndCache(...)`:** This is the on-demand computation for cache misses. It fetches entity details, then conditionally calls `graphService.getRelationships()` and `reasoningService.buildTimeline()` based on the query's needs. The results are then stored in Redis with a `CACHE_TTL` (1 hour).
    *   **`trackEntityUsage(...)`:** Records when an entity's insights are accessed, incrementing a usage count and updating a `lastUsed` timestamp in Redis. This data helps identify "popular" entities.
    *   **`getPopularEntities(...)`:** Retrieves entities that have been used above a `POPULAR_ENTITY_THRESHOLD`, sorted by usage and recency. This is used by the pre-computation job.
    *   **`invalidateEntity(entityName)`:** Crucially, this function deletes an entity's insights from the cache. It's called by `memory-extractor.service.ts` when an entity's underlying data changes, ensuring that stale insights are not served.

2.  **`smartPrecomputeJob.ts`:**
    *   This hourly background job proactively computes and caches insights.
    *   It identifies "active users" (those with recent chat activity) and their "popular entities" (using `smartCacheService.getPopularEntities()`).
    *   For these popular entities, it computes and caches their graph relationships and timeline narratives, ensuring that frequently accessed complex information is always fresh and readily available, even before a user explicitly asks for it.
    *   Includes `cleanupCacheJob()` to remove tracking for entities not used in 30 days.

### III. Extracting Memory Model Prompt Logic (`triplet_extraction.job.ts`)

The goal of memory extraction is to transform unstructured text (user messages, assistant responses, ingested memories) into a structured knowledge graph of entities and their relationships.

1.  **Job Trigger:** The `extractTriplets` job runs periodically, or `extractTripletsForUser` can be manually triggered. It identifies new, unprocessed memories and chat messages.
2.  **Prompt Construction:** For each piece of content, a highly specific prompt is constructed for the LLM:
    ```typescript
    `From the following text, extract knowledge triplets in the format of a JSON array of objects: [{ "subject": "", "predicate": "", "object": "" }].
    For each triplet, also include the "sourceId" and "sourceType" from the provided context. The sourceId is "${contentItem.id}" and sourceType is "${contentItem.type}".
    Focus on factual information, relationships between entities, and key actions. Ensure subjects and objects are specific entities.

    Text:
    ${contentItem.content}

    Triplets (JSON array):`
    ```
    *   **Model:** `qwen2.5:1.5B` is used for this task.
    *   **Format Enforcement:** The prompt explicitly requests a JSON array of objects, including `subject`, `predicate`, `object`, `sourceId`, and `sourceType`. This structured output is crucial for building the knowledge graph.
3.  **LLM Interaction:** The prompt is sent to `llmService.generateCompletion()`.
4.  **Response Parsing:** The LLM's response is parsed to extract the JSON array of triplets. Robust error handling is in place to deal with malformed JSON or empty responses.
5.  **Knowledge Graph Update:**
    *   For each extracted triplet, the system `upserts` (updates if exists, inserts if new) `Entity` records for both the subject and object.
    *   It then creates an `EntityLink` record, connecting the subject and object entities via the `predicate` (role), and linking it back to the original `memoryId` or `chatMessageId` that provided the source.
6.  **Status Update:** The `isTripletExtracted` flag on the original memory or chat message is set to `true`.
7.  **Cache Invalidation:** After successfully extracting triplets and updating entities, `smartCacheService.invalidateEntity()` is called for all detected entities. This ensures that any cached insights related to these entities are cleared, forcing a re-computation with the latest, most accurate information when next requested by a complex query.

This intricate interplay of services ensures that the system can handle diverse queries efficiently, maintain a rich and up-to-date knowledge graph, and provide intelligent, context-aware responses.